# from huggingface_hub import login
# your_token = "INPUT YOUR TOKEN HERE"
# login(your_token)

import sys
import os
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½config.envé…ç½®æ–‡ä»¶
config_path = Path(__file__).parent.parent / "config.env"
load_dotenv(config_path)


import csv
from tqdm import trange
from minirag import MiniRAG, QueryParam
from minirag.llm import (
    hf_model_complete,
    hf_embed,
)
from minirag.utils import EmbeddingFunc
from transformers import AutoModel, AutoTokenizer

EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

import argparse


def get_args():
    parser = argparse.ArgumentParser(description="MiniRAG")
    parser.add_argument("--model", type=str, default="PHI")
    parser.add_argument("--outputpath", type=str, default="./logs/Default_output.csv")
    parser.add_argument("--workingdir", type=str, default="./LiHua-World")
    parser.add_argument("--datapath", type=str, default="./dataset/LiHua-World/data/")
    parser.add_argument(
        "--querypath", type=str, default="./dataset/LiHua-World/qa/query_set.csv"
    )
    # å¹¶è¡Œä¼˜åŒ–å‚æ•° - ä»config.envè¯»å–é»˜è®¤å€¼
    parser.add_argument("--enable-parallel", action="store_true", 
                       help="å¯ç”¨å¹¶è¡Œå®ä½“æå–")
    parser.add_argument("--max-concurrent", type=int, 
                       default=int(os.getenv("PARALLEL_ENTITY_EXTRACTION_MAX_CONCURRENT", 16)),
                       help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--batch-size", type=int, 
                       default=int(os.getenv("PARALLEL_ENTITY_EXTRACTION_BATCH_SIZE", 10)),
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--enable-smart-batching", action="store_true",
                       help="å¯ç”¨æ™ºèƒ½æ‰¹å¤„ç†")
    args = parser.parse_args()
    return args


args = get_args()


# ä»config.envè¯»å–LLMé…ç½®
LLM_BINDING = os.getenv("LLM_BINDING", "openai")
LLM_BINDING_HOST = os.getenv("LLM_BINDING_HOST", "https://api.deepseek.com")
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek-chat")

# å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†æ¨¡å‹ï¼Œåˆ™è¦†ç›–config.envä¸­çš„è®¾ç½®
if args.model != "PHI":  # PHIæ˜¯é»˜è®¤å€¼ï¼Œå¦‚æœæŒ‡å®šå…¶ä»–æ¨¡å‹åˆ™ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.model == "GLM":
        LLM_MODEL = "THUDM/glm-edge-1.5b-chat"
    elif args.model == "MiniCPM":
        LLM_MODEL = "openbmb/MiniCPM3-4B"
    elif args.model == "qwen":
        LLM_MODEL = "Qwen/Qwen2.5-3B-Instruct"
    elif args.model == "PHI":
        LLM_MODEL = "microsoft/Phi-3.5-mini-instruct"
    else:
        # å…è®¸ç”¨æˆ·æŒ‡å®šä»»æ„æ¨¡å‹åç§°
        LLM_MODEL = args.model
        print(f"ğŸ“ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹: {LLM_MODEL}")

print(f"ğŸ”§ LLMé…ç½®:")
print(f"   - ç»‘å®šç±»å‹: {LLM_BINDING}")
print(f"   - æœåŠ¡å™¨åœ°å€: {LLM_BINDING_HOST}")
print(f"   - æ¨¡å‹åç§°: {LLM_MODEL}")
print(f"   - é…ç½®æ¥æº: {'å‘½ä»¤è¡Œè¦†ç›–' if args.model != 'PHI' else 'config.env'}")

WORKING_DIR = args.workingdir
DATA_PATH = args.datapath
QUERY_PATH = args.querypath
OUTPUT_PATH = args.outputpath
print("=" * 60)
print("MiniRAG QA è„šæœ¬é…ç½®")
print("=" * 60)
print(f"LLMæ¨¡å‹: {LLM_MODEL}")
print(f"å·¥ä½œç›®å½•: {WORKING_DIR}")
print(f"æ•°æ®è·¯å¾„: {DATA_PATH}")
print(f"æŸ¥è¯¢æ–‡ä»¶: {QUERY_PATH}")
print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}")
print(f"é…ç½®æ–‡ä»¶: {config_path}")
print("=" * 60)

# æ˜¾ç¤ºä»config.envè¯»å–çš„å…³é”®é…ç½®
print("ğŸ“‹ ä»config.envè¯»å–çš„é…ç½®:")
print(f"   - æœ€å¤§Tokenæ•°: {os.getenv('MAX_TOKENS', '200')}")
print(f"   - åµŒå…¥ç»´åº¦: {os.getenv('EMBEDDING_DIM', '384')}")
print(f"   - æœ€å¤§åµŒå…¥Token: {os.getenv('MAX_EMBED_TOKENS', '1000')}")
print(f"   - æœ€å¤§å¹¶è¡Œæ’å…¥: {os.getenv('MAX_PARALLEL_INSERT', '2')}")
print(f"   - åµŒå…¥æ‰¹å¤„ç†å¤§å°: {os.getenv('EMBEDDING_BATCH_NUM', '32')}")
print(f"   - åµŒå…¥å‡½æ•°æœ€å¤§å¼‚æ­¥æ•°: {os.getenv('EMBEDDING_FUNC_MAX_ASYNC', '16')}")
print(f"   - LLMæ¨¡å‹æœ€å¤§å¼‚æ­¥æ•°: {os.getenv('LLM_MODEL_MAX_ASYNC', '16')}")
print("=" * 60)


if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

# æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å’Œç¯å¢ƒå˜é‡é…ç½®å¹¶è¡Œä¼˜åŒ–
parallel_config = {}

# æ£€æŸ¥æ˜¯å¦å¯ç”¨å¹¶è¡Œä¼˜åŒ–ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼‰
enable_parallel = args.enable_parallel or os.getenv("ENABLE_PARALLEL_ENTITY_EXTRACTION", "true").lower() == "true"

if enable_parallel:
    parallel_config.update({
        "enable_parallel_entity_extraction": True,
        "parallel_entity_extraction_max_concurrent": args.max_concurrent,
        "parallel_entity_extraction_batch_size": args.batch_size,
        "enable_smart_batching": args.enable_smart_batching or os.getenv("ENABLE_SMART_BATCHING", "true").lower() == "true",
    })
    print(f"ğŸš€ å¯ç”¨å¹¶è¡Œä¼˜åŒ–:")
    print(f"   - æœ€å¤§å¹¶å‘æ•°: {args.max_concurrent} (æ¥è‡ª: {'å‘½ä»¤è¡Œ' if args.enable_parallel else 'config.env'})")
    print(f"   - æ‰¹å¤„ç†å¤§å°: {args.batch_size} (æ¥è‡ª: {'å‘½ä»¤è¡Œ' if args.enable_parallel else 'config.env'})")
    print(f"   - æ™ºèƒ½æ‰¹å¤„ç†: {parallel_config['enable_smart_batching']}")
    print(f"   - é…ç½®æ¥æº: config.env + å‘½ä»¤è¡Œè¦†ç›–")
else:
    print("âš¡ ä½¿ç”¨é»˜è®¤ä¸²è¡Œæ¨¡å¼")

# æ ¹æ®config.envä¸­çš„ç»‘å®šç±»å‹é€‰æ‹©LLMå‡½æ•°
def get_llm_function():
    """æ ¹æ®é…ç½®é€‰æ‹©LLMå‡½æ•°"""
    if LLM_BINDING == "openai":
        # ä½¿ç”¨OpenAIå…¼å®¹çš„API
        try:
            from openai import OpenAI
            client = OpenAI(
                base_url=LLM_BINDING_HOST,
                api_key=os.getenv("LLM_BINDING_API_KEY")
            )
            
            async def openai_llm_func(prompt, **kwargs):
                try:
                    # è¿‡æ»¤æ‰MiniRAGç‰¹æœ‰çš„å‚æ•°
                    filtered_kwargs = {}
                    for key, value in kwargs.items():
                        if key not in ['hashing_kv', 'keyword_extraction', 'system_prompt', 'history_messages']:
                            filtered_kwargs[key] = value
                    
                    response = client.chat.completions.create(
                        model=LLM_MODEL,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=int(os.getenv("MAX_TOKENS", 200)),
                        **filtered_kwargs
                    )
                    return response.choices[0].message.content
                except Exception as e:
                    print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
                    return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
            
            return openai_llm_func
            
        except ImportError:
            print("âš ï¸ OpenAIåº“æœªå®‰è£…ï¼Œå›é€€åˆ°HuggingFaceæ¨¡å‹")
            return hf_model_complete
    
    elif LLM_BINDING == "ollama":
        # ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹
        try:
            from minirag.llm import ollama_complete
            
            async def ollama_llm_func(prompt, **kwargs):
                return await ollama_complete(prompt, model=LLM_MODEL, **kwargs)
            
            return ollama_llm_func
            
        except ImportError:
            print("âš ï¸ Ollamaæ”¯æŒæœªå®‰è£…ï¼Œå›é€€åˆ°HuggingFaceæ¨¡å‹")
            return hf_model_complete
    
    else:
        # é»˜è®¤ä½¿ç”¨HuggingFaceæ¨¡å‹
        print(f"âš ï¸ æœªçŸ¥çš„LLMç»‘å®šç±»å‹: {LLM_BINDING}ï¼Œä½¿ç”¨HuggingFaceæ¨¡å‹")
        return hf_model_complete

# è·å–LLMå‡½æ•°
llm_func = get_llm_function()

rag = MiniRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_func,
    llm_model_max_token_size=int(os.getenv("MAX_TOKENS", 200)),
    llm_model_name=LLM_MODEL,
    # ä»config.envè¯»å–é«˜çº§é…ç½®
    max_parallel_insert=int(os.getenv("MAX_PARALLEL_INSERT", 2)),
    embedding_batch_num=int(os.getenv("EMBEDDING_BATCH_NUM", 32)),
    embedding_func_max_async=int(os.getenv("EMBEDDING_FUNC_MAX_ASYNC", 16)),
    llm_model_max_async=int(os.getenv("LLM_MODEL_MAX_ASYNC", 16)),
    # åµŒå…¥å‡½æ•°é…ç½®
    embedding_func=EmbeddingFunc(
        embedding_dim=int(os.getenv("EMBEDDING_DIM", 384)),
        max_token_size=int(os.getenv("MAX_EMBED_TOKENS", 1000)),
        func=lambda texts: hf_embed(
            texts,
            tokenizer=AutoTokenizer.from_pretrained(EMBEDDING_MODEL),
            embed_model=AutoModel.from_pretrained(EMBEDDING_MODEL),
        ),
    ),
    **parallel_config  # åº”ç”¨å¹¶è¡Œé…ç½®
)

# Now QA
QUESTION_LIST = []
GA_LIST = []
with open(QUERY_PATH, mode="r", encoding="utf-8") as question_file:
    reader = csv.DictReader(question_file)
    for row in reader:
        QUESTION_LIST.append(row["Question"])
        GA_LIST.append(row["Gold Answer"])


def run_experiment(output_path):
    headers = ["Question", "Gold Answer", "minirag"]

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    q_already = []
    if os.path.exists(output_path):
        with open(output_path, mode="r", encoding="utf-8") as question_file:
            reader = csv.DictReader(question_file)
            for row in reader:
                q_already.append(row["Question"])

    row_count = len(q_already)
    print("row_count", row_count)

    with open(output_path, mode="a", newline="", encoding="utf-8") as log_file:
        writer = csv.writer(log_file)
        if row_count == 0:
            writer.writerow(headers)

        for QUESTIONid in trange(row_count, len(QUESTION_LIST)):  #
            QUESTION = QUESTION_LIST[QUESTIONid]
            Gold_Answer = GA_LIST[QUESTIONid]
            print()
            print("QUESTION", QUESTION)
            print("Gold_Answer", Gold_Answer)

            try:
                minirag_answer = (
                    rag.query(QUESTION, param=QueryParam(mode="mini"))
                    .replace("\n", "")
                    .replace("\r", "")
                )
            except Exception as e:
                print("Error in minirag_answer", e)
                minirag_answer = "Error"

            writer.writerow([QUESTION, Gold_Answer, minirag_answer])

    print(f"Experiment data has been recorded in the file: {output_path}")


if __name__ == "__main__":
    print("\nğŸ“– ä½¿ç”¨è¯´æ˜:")
    print("åŸºç¡€ç”¨æ³•:")
    print("  python Step_1_QA.py --model PHI")
    print("\nä½¿ç”¨config.envä¸­çš„æ¨¡å‹:")
    print("  python Step_1_QA.py")
    print("\næŒ‡å®šä»»æ„æ¨¡å‹åç§°:")
    print("  python Step_1_QA.py --model your-model-name")
    print("\nå¯ç”¨å¹¶è¡Œä¼˜åŒ–:")
    print("  python Step_1_QA.py --enable-parallel --max-concurrent 8")
    print("\nå®Œæ•´å‚æ•°:")
    print("  python Step_1_QA.py --model your-model --enable-parallel --max-concurrent 8 --batch-size 10 --enable-smart-batching")
    print("\nğŸ“ é…ç½®è¯´æ˜:")
    print("  - è„šæœ¬ä¼šè‡ªåŠ¨è¯»å– config.env ä¸­çš„é…ç½®")
    print("  - å‘½ä»¤è¡Œå‚æ•°ä¼šè¦†ç›– config.env ä¸­çš„è®¾ç½®")
    print("  - å¹¶è¡Œä¼˜åŒ–é»˜è®¤ä» config.env å¯ç”¨")
    print("  - æ”¯æŒä»»æ„æ¨¡å‹åç§°ï¼Œä¸å†é™åˆ¶é¢„å®šä¹‰é€‰é¡¹")
    print("\n" + "=" * 60)
    
    run_experiment(OUTPUT_PATH)
