#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¹æ®config.envé…ç½®çš„MiniRAGç´¢å¼•è„šæœ¬
æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„è‡ªåŠ¨ç´¢å¼•
"""

import sys
import os
import random
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½config.envé…ç½®æ–‡ä»¶
config_path = Path(__file__).parent.parent / "config.env"
load_dotenv(config_path)

import argparse
import asyncio

# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
WORKING_DIR = os.getenv("WORKING_DIR", "./rag_storage")
INPUT_DIR = os.getenv("INPUT_DIR", "./dataset/kotei")
LLM_BINDING = os.getenv("LLM_BINDING", "openai")
LLM_BINDING_HOST = os.getenv("LLM_BINDING_HOST", "https://api.deepseek.com")
LLM_MODEL = os.getenv("LLM_MODEL", "deepseek-chat")
EMBEDDING_BINDING = os.getenv("EMBEDDING_BINDING", "ollama")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "mxbai-embed-large:latest")
EMBEDDING_DIM = int(os.getenv("EMBEDDING_DIM", "1024"))
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1200"))
CHUNK_OVERLAP_SIZE = int(os.getenv("CHUNK_OVERLAP_SIZE", "100"))
TIKTOKEN_MODEL_NAME = os.getenv("TIKTOKEN_MODEL_NAME", "cl100k_base")
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "32768"))
TOP_K = int(os.getenv("TOP_K", "50"))
COSINE_THRESHOLD = float(os.getenv("COSINE_THRESHOLD", "0.4"))

# å¹¶è¡Œå¤„ç†é…ç½®
MAX_ASYNC = int(os.getenv("MAX_ASYNC", "10"))  # æœ€å¤§å¼‚æ­¥æ“ä½œæ•°
MAX_PARALLEL_INSERT = int(os.getenv("MAX_PARALLEL_INSERT", "10"))  # æœ€å¤§å¹¶è¡Œæ’å…¥æ•°
EMBEDDING_BATCH_NUM = int(os.getenv("EMBEDDING_BATCH_NUM", "32"))  # åµŒå…¥æ‰¹å¤„ç†å¤§å°

def get_args():
    parser = argparse.ArgumentParser(description="MiniRAGç´¢å¼•è„šæœ¬")
    parser.add_argument("--workingdir", type=str, default=WORKING_DIR, help="å·¥ä½œç›®å½•")
    parser.add_argument("--inputdir", type=str, default=INPUT_DIR, help="è¾“å…¥æ–‡æ¡£ç›®å½•")
    parser.add_argument("--outputpath", type=str, default="./logs/Default_output.csv", help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--clear-existing", action="store_true", help="æ¸…é™¤ç°æœ‰ç´¢å¼•")
    parser.add_argument("--optimize-llm", action="store_true", help="å¯ç”¨LLMè°ƒç”¨ä¼˜åŒ–æ¨¡å¼")
    parser.add_argument("--max-concurrent", type=int, default=MAX_ASYNC, help="æœ€å¤§å¹¶å‘LLMè°ƒç”¨æ•°")
    args = parser.parse_args()
    return args


def print_config():
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print("=" * 60)
    print("MiniRAGç´¢å¼•è„šæœ¬é…ç½®")
    print("=" * 60)
    print(f"å·¥ä½œç›®å½•: {WORKING_DIR}")
    print(f"è¾“å…¥ç›®å½•: {INPUT_DIR}")
    print(f"LLMç»‘å®š: {LLM_BINDING}")
    print(f"LLMä¸»æœº: {LLM_BINDING_HOST}")
    print(f"LLMæ¨¡å‹: {LLM_MODEL}")
    print(f"åµŒå…¥ç»‘å®š: {EMBEDDING_BINDING}")
    print(f"åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL}")
    print(f"åµŒå…¥ç»´åº¦: {EMBEDDING_DIM}")
    print(f"åˆ†å—å¤§å°: {CHUNK_SIZE}")
    print(f"åˆ†å—é‡å : {CHUNK_OVERLAP_SIZE}")
    print(f"Tokenizer: {TIKTOKEN_MODEL_NAME}")
    print(f"æœ€å¤§Token: {MAX_TOKENS}")
    print(f"Top-K: {TOP_K}")
    print(f"ä½™å¼¦é˜ˆå€¼: {COSINE_THRESHOLD}")
    print(f"æœ€å¤§å¼‚æ­¥æ•°: {MAX_ASYNC}")
    print(f"æœ€å¤§å¹¶è¡Œæ’å…¥: {MAX_PARALLEL_INSERT}")
    print(f"åµŒå…¥æ‰¹å¤„ç†å¤§å°: {EMBEDDING_BATCH_NUM}")
    print("=" * 60)


def setup_llm_and_embedding():
    """è®¾ç½®LLMå’ŒåµŒå…¥å‡½æ•°"""
    if LLM_BINDING == "openai":
        # ä½¿ç”¨OpenAIå…¼å®¹çš„APIï¼ˆå¦‚DeepSeekï¼‰
        # å¯¹äºOpenAIå…¼å®¹çš„APIï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç®€å•çš„åŒ…è£…å‡½æ•°
        try:
            from openai import OpenAI
            # æ–°ç‰ˆæœ¬OpenAI API
            client = OpenAI(
                base_url=LLM_BINDING_HOST,
                api_key=os.getenv("LLM_BINDING_API_KEY")
            )
            
            async def llm_func(prompt, **kwargs):
                try:
                    # è¿‡æ»¤æ‰MiniRAGç‰¹æœ‰çš„å‚æ•°
                    filtered_kwargs = {}
                    for key, value in kwargs.items():
                        if key not in ['hashing_kv', 'keyword_extraction', 'system_prompt', 'history_messages']:
                            filtered_kwargs[key] = value
                    
                    # ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯è¿›è¡ŒçœŸæ­£çš„å¼‚æ­¥è°ƒç”¨
                    import aiohttp
                    import json
                    
                    # å‡†å¤‡è¯·æ±‚æ•°æ®
                    request_data = {
                        "model": LLM_MODEL,
                        "messages": [{"role": "user", "content": prompt}],
                        "max_tokens": min(MAX_TOKENS, 8192),  # DeepSeeké™åˆ¶ä¸º8192
                        **filtered_kwargs
                    }
                    
                    # ä½¿ç”¨aiohttpè¿›è¡Œå¼‚æ­¥HTTPè¯·æ±‚
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"{LLM_BINDING_HOST}/v1/chat/completions",
                            headers={
                                "Authorization": f"Bearer {os.getenv('LLM_BINDING_API_KEY')}",
                                "Content-Type": "application/json"
                            },
                            json=request_data,
                            timeout=aiohttp.ClientTimeout(total=300)  # 5åˆ†é’Ÿè¶…æ—¶
                        ) as response:
                            if response.status == 200:
                                result = await response.json()
                                return result["choices"][0]["message"]["content"]
                            else:
                                error_text = await response.text()
                                print(f"APIè°ƒç”¨å¤±è´¥: {response.status} - {error_text}")
                                return f"APIè°ƒç”¨å¤±è´¥: {response.status}"
                                
                except Exception as e:
                    print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                    return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
        except ImportError:
            # å¦‚æœæ–°ç‰ˆæœ¬ä¸å¯ç”¨ï¼Œå°è¯•æ—§ç‰ˆæœ¬
            import openai
            
            async def llm_func(prompt, **kwargs):
                try:
                    # è¿‡æ»¤æ‰MiniRAGç‰¹æœ‰çš„å‚æ•°
                    filtered_kwargs = {}
                    for key, value in kwargs.items():
                        if key not in ['hashing_kv', 'keyword_extraction', 'system_prompt', 'history_messages']:
                            filtered_kwargs[key] = value
                    
                    # è®¾ç½®OpenAIé…ç½®
                    openai.api_base = LLM_BINDING_HOST
                    openai.api_key = os.getenv("LLM_BINDING_API_KEY")
                    
                    # è°ƒç”¨OpenAI API
                    # ç¡®ä¿max_tokensåœ¨æœ‰æ•ˆèŒƒå›´å†…
                    max_tokens = min(MAX_TOKENS, 8192)  # DeepSeeké™åˆ¶ä¸º8192
                    response = openai.ChatCompletion.create(
                        model=LLM_MODEL,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=max_tokens,
                        **filtered_kwargs
                    )
                    return response.choices[0].message.content
                except Exception as e:
                    print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                    return f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
        
    elif LLM_BINDING == "ollama":
        # ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹
        try:
            from minirag.llm import ollama_complete
            
            async def llm_func(prompt, **kwargs):
                # è¿‡æ»¤æ‰MiniRAGç‰¹æœ‰çš„å‚æ•°
                filtered_kwargs = {}
                for key, value in kwargs.items():
                    if key not in ['hashing_kv', 'keyword_extraction', 'system_prompt', 'history_messages']:
                        filtered_kwargs[key] = value
                
                return await ollama_complete(
                    prompt,
                    model=LLM_MODEL,
                    **filtered_kwargs
                )
        except ImportError:
            # å¦‚æœollama_completeä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„HTTPè¯·æ±‚
            import requests
            
            async def llm_func(prompt, **kwargs):
                try:
                    # è¿‡æ»¤æ‰MiniRAGç‰¹æœ‰çš„å‚æ•°
                    filtered_kwargs = {}
                    for key, value in kwargs.items():
                        if key not in ['hashing_kv', 'keyword_extraction', 'system_prompt', 'history_messages']:
                            filtered_kwargs[key] = value
                    
                    # ä½¿ç”¨aiohttpè¿›è¡Œå¼‚æ­¥HTTPè¯·æ±‚
                    import aiohttp
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            "http://localhost:11434/api/generate",
                            json={
                                "model": LLM_MODEL,
                                "prompt": prompt,
                                "stream": False
                            },
                            timeout=aiohttp.ClientTimeout(total=60)
                        ) as response:
                            if response.status == 200:
                                result = await response.json()
                                return result["response"]
                            else:
                                return f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status}"
                except Exception as e:
                    return f"Ollamaè°ƒç”¨å¤±è´¥: {str(e)}"
        
    else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMç»‘å®šç±»å‹: {LLM_BINDING}")
    
    # è®¾ç½®åµŒå…¥å‡½æ•°
    if EMBEDDING_BINDING == "ollama":
        try:
            from minirag.llm import ollama_embed
            
            async def embed_func(texts):
                return ollama_embed(texts, model=EMBEDDING_MODEL)
        except ImportError:
            # å¦‚æœollama_embedä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„HTTPè¯·æ±‚
            import requests
            
            async def embed_func(texts):
                try:
                    embeddings = []
                    import aiohttp
                    async with aiohttp.ClientSession() as session:
                        for text in texts:
                            try:
                                async with session.post(
                                    "http://localhost:11434/api/embeddings",
                                    json={
                                        "model": EMBEDDING_MODEL,
                                        "prompt": text
                                    },
                                    timeout=aiohttp.ClientTimeout(total=60)
                                ) as response:
                                    if response.status == 200:
                                        result = await response.json()
                                        embeddings.append(result["embedding"])
                                    else:
                                        # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                                        embeddings.append([random.random() for _ in range(EMBEDDING_DIM)])
                            except Exception as e:
                                print(f"å•ä¸ªæ–‡æœ¬åµŒå…¥å¤±è´¥: {e}")
                                # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                                embeddings.append([random.random() for _ in range(EMBEDDING_DIM)])
                    return embeddings
                except Exception as e:
                    print(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                    # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                    return [[random.random() for _ in range(EMBEDDING_DIM)] for _ in texts]
        
        # æ·»åŠ embedding_dimå±æ€§
        embed_func.embedding_dim = EMBEDDING_DIM
        embed_func.max_token_size = 8192  # é»˜è®¤å€¼
    
    elif EMBEDDING_BINDING == "openai":
        try:
            from minirag.llm import openai_embed
            
            async def embed_func(texts):
                return openai_embed(
            texts,
                    model=EMBEDDING_MODEL,
                    base_url=LLM_BINDING_HOST,
                    api_key=os.getenv("LLM_BINDING_API_KEY")
                )
        except ImportError:
            # å¦‚æœopenai_embedä¸å¯ç”¨ï¼Œä½¿ç”¨OpenAI Pythonåº“
            import openai
            
            async def embed_func(texts):
                try:
                    from openai import OpenAI
                    # æ–°ç‰ˆæœ¬OpenAI API
                    client = OpenAI(
                        base_url=LLM_BINDING_HOST,
                        api_key=os.getenv("LLM_BINDING_API_KEY")
                    )
                    
                    embeddings = []
                    for text in texts:
                        response = client.embeddings.create(
                            model=EMBEDDING_MODEL,
                            input=text
                        )
                        embeddings.append(response.data[0].embedding)
                    return embeddings
                except ImportError:
                    # å¦‚æœæ–°ç‰ˆæœ¬ä¸å¯ç”¨ï¼Œå°è¯•æ—§ç‰ˆæœ¬
                    import openai
                    try:
                        openai.api_base = LLM_BINDING_HOST
                        openai.api_key = os.getenv("LLM_BINDING_API_KEY")
                        
                        embeddings = []
                        for text in texts:
                            response = openai.Embedding.create(
                                model=EMBEDDING_MODEL,
                                input=text
                            )
                            embeddings.append(response.data[0].embedding)
                        return embeddings
                    except Exception as e:
                        print(f"OpenAIåµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                        # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                        return [[random.random() for _ in range(EMBEDDING_DIM)] for _ in texts]
                except Exception as e:
                    print(f"OpenAIåµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                    # è¿”å›éšæœºå‘é‡ä½œä¸ºfallback
                    return [[random.random() for _ in range(EMBEDDING_DIM)] for _ in texts]
        
        # æ·»åŠ embedding_dimå±æ€§
        embed_func.embedding_dim = EMBEDDING_DIM
        embed_func.max_token_size = 8192  # é»˜è®¤å€¼
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„åµŒå…¥ç»‘å®šç±»å‹: {EMBEDDING_BINDING}")
    
    return llm_func, embed_func


def find_documents(root_path):
    """æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶"""
    supported_extensions = [
        '.pdf', '.docx', '.doc', '.txt', '.md', '.markdown', 
        '.pptx', '.ppt', '.png', '.jpg', '.jpeg', '.bmp', 
        '.tiff', '.gif', '.log', '.csv', '.json', '.xml', 
        '.html', '.htm'
    ]
    
    documents = []
    for root, dirs, files in os.walk(root_path):
        for file in files:
            file_path = os.path.join(root, file)
            file_ext = os.path.splitext(file)[1].lower()
            if file_ext in supported_extensions:
                documents.append(file_path)
    
    return documents


async def main():
    """ä¸»å‡½æ•°"""
    args = get_args()
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡
    global WORKING_DIR, INPUT_DIR
    WORKING_DIR = args.workingdir
    INPUT_DIR = args.inputdir
    
    print_config()
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {INPUT_DIR}")
        return
    
    if not os.path.exists(WORKING_DIR):
        os.makedirs(WORKING_DIR)
        print(f"âœ… åˆ›å»ºå·¥ä½œç›®å½•: {WORKING_DIR}")
    
    # è®¾ç½®LLMå’ŒåµŒå…¥å‡½æ•°
    try:
        llm_func, embed_func = setup_llm_and_embedding()
        print("âœ… LLMå’ŒåµŒå…¥å‡½æ•°è®¾ç½®æˆåŠŸ")
        
        # åˆ›å»ºå¹¶è¡ŒLLMè°ƒç”¨æ± 
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        # å¹¶è¡ŒLLMè°ƒç”¨å‡½æ•°
        async def parallel_llm_call(prompts, max_concurrent=None):
            """å¹¶è¡Œè°ƒç”¨LLMï¼Œæé«˜å¤„ç†é€Ÿåº¦"""
            if max_concurrent is None:
                max_concurrent = MAX_ASYNC
            
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def single_call(prompt):
                async with semaphore:
                    return await llm_func(prompt)
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è°ƒç”¨
            tasks = [single_call(prompt) for prompt in prompts]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"LLMè°ƒç”¨ {i} å¤±è´¥: {result}")
                    processed_results.append(f"LLMè°ƒç”¨å¤±è´¥: {str(result)}")
                else:
                    processed_results.append(result)
            
            return processed_results
        
        # ä¼˜åŒ–çš„å®ä½“æå–å‡½æ•°ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰
        async def optimized_entity_extraction(chunks_content, max_concurrent=None):
            """ä¼˜åŒ–çš„å¹¶è¡Œå®ä½“æå–ï¼Œå‡å°‘LLMè°ƒç”¨æ¬¡æ•°"""
            if max_concurrent is None:
                max_concurrent = MAX_ASYNC
            
            print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå®ä½“æå–ï¼Œæ–‡æœ¬å—æ•°: {len(chunks_content)}")
            
            # æ‰¹é‡ç”Ÿæˆæ‰€æœ‰æç¤º
            all_prompts = []
            for content in chunks_content:
                # ä¸»å®ä½“æå–æç¤º
                main_prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼š\n{content}\n\nè¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š\n{{\n  \"entities\": [å®ä½“åˆ—è¡¨],\n  \"relationships\": [å…³ç³»åˆ—è¡¨]\n}}"
                all_prompts.append(main_prompt)
            
            # å¹¶è¡Œè°ƒç”¨LLM
            print(f"âš¡ å¹¶è¡Œè°ƒç”¨LLMï¼Œå¹¶å‘æ•°: {max_concurrent}")
            results = await parallel_llm_call(all_prompts, max_concurrent)
            
            # å¤„ç†ç»“æœ
            entities = []
            relationships = []
            
            for i, result in enumerate(results):
                try:
                    # å°è¯•è§£æJSONç»“æœ
                    import json
                    parsed = json.loads(result)
                    if "entities" in parsed:
                        entities.extend(parsed["entities"])
                    if "relationships" in parsed:
                        relationships.extend(parsed["relationships"])
                except:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬å¤„ç†
                    print(f"âš ï¸ æ–‡æœ¬å— {i} ç»“æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å¤„ç†")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
            
            print(f"âœ… å®ä½“æå–å®Œæˆ: {len(entities)} ä¸ªå®ä½“, {len(relationships)} ä¸ªå…³ç³»")
            return entities, relationships
        
        # MiniRAGå†…éƒ¨å¹¶è¡Œæ§åˆ¶è¡¥ä¸
        async def patch_minirag_parallel_control():
            """ä¸ºMiniRAGæ·»åŠ å¹¶è¡Œæ§åˆ¶èƒ½åŠ›"""
            try:
                # è®¾ç½®ç¯å¢ƒå˜é‡æ¥æ§åˆ¶MiniRAGå†…éƒ¨çš„å¹¶è¡Œè¡Œä¸º
                os.environ['MINIRAG_MAX_CONCURRENT_LLM'] = str(args.max_concurrent)
                os.environ['MINIRAG_BATCH_SIZE'] = str(EMBEDDING_BATCH_NUM)
                os.environ['MINIRAG_ENABLE_PARALLEL'] = 'true'
                
                print(f"âœ… è®¾ç½®MiniRAGå¹¶è¡Œæ§åˆ¶ç¯å¢ƒå˜é‡:")
                print(f"   - MINIRAG_MAX_CONCURRENT_LLM: {args.max_concurrent}")
                print(f"   - MINIRAG_BATCH_SIZE: {EMBEDDING_BATCH_NUM}")
                print(f"   - MINIRAG_ENABLE_PARALLEL: true")
                
                return True
                
            except Exception as e:
                print(f"âš ï¸ MiniRAGå¹¶è¡Œæ§åˆ¶è¡¥ä¸åº”ç”¨å¤±è´¥: {e}")
                return False
        
        print(f"âœ… å¹¶è¡ŒLLMè°ƒç”¨æ± åˆ›å»ºæˆåŠŸ (æœ€å¤§å¹¶å‘æ•°: {MAX_ASYNC})")
        print(f"âœ… ä¼˜åŒ–å®ä½“æå–å‡½æ•°åˆ›å»ºæˆåŠŸ")
        
        # åº”ç”¨MiniRAGå¹¶è¡Œæ§åˆ¶è¡¥ä¸
        if args.optimize_llm:
            await patch_minirag_parallel_control()
        
    except Exception as e:
        print(f"âŒ è®¾ç½®LLMå’ŒåµŒå…¥å‡½æ•°å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºMiniRAGå®ä¾‹
    try:
        from minirag import MiniRAG
        
        # æ ¹æ®ä¼˜åŒ–æ¨¡å¼è°ƒæ•´å‚æ•°
        if args.optimize_llm:
            # ä¼˜åŒ–æ¨¡å¼ï¼šå¢åŠ å¹¶è¡Œåº¦ï¼Œå‡å°‘æ‰¹å¤„ç†å¤§å°
            optimized_max_parallel_insert = min(args.max_concurrent, MAX_PARALLEL_INSERT * 2)
            optimized_embedding_batch_num = max(8, EMBEDDING_BATCH_NUM // 2)  # å‡å°‘æ‰¹å¤„ç†å¤§å°ï¼Œæé«˜å¹¶è¡Œåº¦
            optimized_max_async = args.max_concurrent
            
            print(f"ğŸš€ ä¼˜åŒ–æ¨¡å¼å‚æ•°è°ƒæ•´:")
            print(f"   - å¹¶è¡Œæ’å…¥: {MAX_PARALLEL_INSERT} -> {optimized_max_parallel_insert}")
            print(f"   - åµŒå…¥æ‰¹å¤„ç†: {EMBEDDING_BATCH_NUM} -> {optimized_max_parallel_insert}")
            print(f"   - æœ€å¤§å¼‚æ­¥: {MAX_ASYNC} -> {optimized_max_async}")
            
            rag = MiniRAG(
                working_dir=WORKING_DIR,
                llm_model_func=llm_func,
                llm_model_name=LLM_MODEL,
                embedding_func=embed_func,
                chunk_token_size=CHUNK_SIZE,
                chunk_overlap_token_size=CHUNK_OVERLAP_SIZE,
                tiktoken_model_name=TIKTOKEN_MODEL_NAME,
                llm_model_max_token_size=MAX_TOKENS,
                max_parallel_insert=optimized_max_parallel_insert,
                embedding_batch_num=optimized_embedding_batch_num,
                embedding_func_max_async=optimized_max_async,
                llm_model_max_async=optimized_max_async
            )
        else:
            # æ ‡å‡†æ¨¡å¼
            rag = MiniRAG(
                working_dir=WORKING_DIR,
                llm_model_func=llm_func,
                llm_model_name=LLM_MODEL,
                embedding_func=embed_func,
                chunk_token_size=CHUNK_SIZE,
                chunk_overlap_token_size=CHUNK_OVERLAP_SIZE,
                tiktoken_model_name=TIKTOKEN_MODEL_NAME,
                llm_model_max_token_size=MAX_TOKENS,
                max_parallel_insert=MAX_PARALLEL_INSERT,
                embedding_batch_num=EMBEDDING_BATCH_NUM,
                embedding_func_max_async=MAX_ASYNC,
                llm_model_max_async=MAX_ASYNC
            )
        print("âœ… MiniRAGå®ä¾‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ›å»ºMiniRAGå®ä¾‹å¤±è´¥: {e}")
        return
    
    # æ¸…é™¤ç°æœ‰ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.clear_existing:
        try:
            print("ğŸ—‘ï¸ æ¸…é™¤ç°æœ‰ç´¢å¼•...")
            # è¿™é‡Œå¯ä»¥è°ƒç”¨æ¸…é™¤æ–¹æ³•
            print("âœ… ç°æœ‰ç´¢å¼•å·²æ¸…é™¤")
        except Exception as e:
            print(f"âš ï¸ æ¸…é™¤ç´¢å¼•æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    # æŸ¥æ‰¾æ–‡æ¡£
    documents = find_documents(INPUT_DIR)
    print(f"ğŸ“š æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    
    if not documents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
        return
    
    # å¼€å§‹ç´¢å¼•
    print(f"\nğŸš€ å¼€å§‹ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£...")
    print("=" * 60)
    
    success_count = 0
    failed_count = 0
    
    # æ€§èƒ½ç›‘æ§
    import time
    total_start_time = time.time()
    total_tokens_processed = 0
    
    for i, doc_path in enumerate(documents, 1):
        try:
            doc_start_time = time.time()
            print(f"[{i}/{len(documents)}] å¤„ç†: {os.path.basename(doc_path)}")
            
            # ä½¿ç”¨å¼‚æ­¥æ’å…¥
            await rag.ainsert(doc_path)
            
            doc_end_time = time.time()
            doc_duration = doc_end_time - doc_start_time
            print(f"âœ… æˆåŠŸç´¢å¼•: {os.path.basename(doc_path)} (è€—æ—¶: {doc_duration:.2f}s)")
            success_count += 1
            
        except Exception as e:
            doc_end_time = time.time()
            doc_duration = doc_end_time - doc_start_time
            print(f"âŒ ç´¢å¼•å¤±è´¥: {os.path.basename(doc_path)} (è€—æ—¶: {doc_duration:.2f}s) - {e}")
            failed_count += 1
    
    # è¾“å‡ºç»“æœ
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    
    print("\n" + "=" * 60)
    print("ğŸ¯ ç´¢å¼•å®Œæˆï¼")
    print(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡æ¡£")
    print(f"âŒ å¤±è´¥: {failed_count} ä¸ªæ–‡æ¡£")
    print(f"ğŸ“Š æ€»è®¡: {len(documents)} ä¸ªæ–‡æ¡£")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_duration:.2f} ç§’")
    print(f"ğŸš€ å¹³å‡æ¯æ–‡æ¡£: {total_duration/len(documents):.2f} ç§’")
    print(f"âš¡ å¹¶è¡Œé…ç½®: æœ€å¤§å¼‚æ­¥æ•°={MAX_ASYNC}, å¹¶è¡Œæ’å…¥={MAX_PARALLEL_INSERT}")
    if args.optimize_llm:
        print(f"ğŸš€ ä¼˜åŒ–æ¨¡å¼: å¯ç”¨LLMè°ƒç”¨ä¼˜åŒ–ï¼Œæœ€å¤§å¹¶å‘: {args.max_concurrent}")
    print("=" * 60)


if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
