#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniRAGå¹¶è¡Œæ§åˆ¶è¡¥ä¸
ç”¨äºä¼˜åŒ–extract_entitieså‡½æ•°çš„LLMè°ƒç”¨å¹¶è¡Œæ€§
"""

import asyncio
import logging
from typing import Dict, List, Any, Tuple
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ParallelEntityExtractor:
    """å¹¶è¡Œå®ä½“æå–å™¨ï¼Œç”¨äºæ›¿æ¢MiniRAGå†…éƒ¨çš„ä¸²è¡Œè°ƒç”¨"""
    
    def __init__(self, llm_func, max_concurrent=4, batch_size=8):
        self.llm_func = llm_func
        self.max_concurrent = max_concurrent
        self.batch_size = batch_size
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def extract_entities_parallel(
        self,
        chunks: Dict[str, Any],
        knowledge_graph_inst,
        entity_vdb,
        entity_name_vdb,
        relationships_vdb,
        global_config: dict
    ):
        """å¹¶è¡Œç‰ˆæœ¬çš„å®ä½“æå–å‡½æ•°"""
        
        ordered_chunks = list(chunks.items())
        total_chunks = len(ordered_chunks)
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå®ä½“æå–ï¼Œæ€»æ–‡æœ¬å—æ•°: {total_chunks}")
        logger.info(f"âš¡ å¹¶è¡Œé…ç½®: æœ€å¤§å¹¶å‘={self.max_concurrent}, æ‰¹å¤„ç†å¤§å°={self.batch_size}")
        
        # åˆ†æ‰¹å¤„ç†æ–‡æœ¬å—
        all_entities = []
        all_relationships = []
        
        for batch_start in range(0, total_chunks, self.batch_size):
            batch_end = min(batch_start + self.batch_size, total_chunks)
            batch_chunks = ordered_chunks[batch_start:batch_end]
            
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_start//self.batch_size + 1}: æ–‡æœ¬å— {batch_start+1}-{batch_end}")
            
            # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
            batch_results = await self._process_batch_parallel(batch_chunks, global_config)
            
            # æ”¶é›†ç»“æœ
            for entities, relationships in batch_results:
                all_entities.extend(entities)
                all_relationships.extend(relationships)
            
            logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: æå–äº† {len(all_entities)} ä¸ªå®ä½“, {len(all_relationships)} ä¸ªå…³ç³»")
        
        logger.info(f"ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼æ€»è®¡: {len(all_entities)} ä¸ªå®ä½“, {len(all_relationships)} ä¸ªå…³ç³»")
        
        # è¿™é‡Œéœ€è¦è°ƒç”¨åŸæœ‰çš„åˆå¹¶å’Œå­˜å‚¨é€»è¾‘
        # ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥ä¿®æ”¹MiniRAGå†…éƒ¨ï¼Œè¿™é‡Œè¿”å›å¤„ç†åçš„æ•°æ®
        return {
            'entities': all_entities,
            'relationships': all_relationships,
            'chunks_processed': total_chunks
        }
    
    async def _process_batch_parallel(self, batch_chunks: List[Tuple], global_config: dict):
        """å¹¶è¡Œå¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ–‡æœ¬å—"""
        
        # åˆ›å»ºæ‰€æœ‰éœ€è¦çš„æç¤º
        all_prompts = []
        for chunk_key, chunk_data in batch_chunks:
            content = chunk_data["content"]
            
            # ç®€åŒ–çš„æç¤ºï¼Œå‡å°‘LLMè°ƒç”¨æ¬¡æ•°
            prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š

æ–‡æœ¬å†…å®¹ï¼š
{content}

è¯·è¿”å›æ ¼å¼ï¼š
{{
  "entities": [
    {{"name": "å®ä½“å", "type": "å®ä½“ç±»å‹", "description": "æè¿°"}}
  ],
  "relationships": [
    {{"source": "æºå®ä½“", "target": "ç›®æ ‡å®ä½“", "relation": "å…³ç³»ç±»å‹"}}
  ]
}}"""
            
            all_prompts.append((chunk_key, prompt))
        
        # å¹¶è¡Œè°ƒç”¨LLM
        async def single_llm_call(chunk_key, prompt):
            async with self.semaphore:
                try:
                    result = await self.llm_func(prompt)
                    return chunk_key, result, None
                except Exception as e:
                    logger.warning(f"LLMè°ƒç”¨å¤±è´¥ {chunk_key}: {e}")
                    return chunk_key, None, str(e)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰LLMè°ƒç”¨
        tasks = [single_llm_call(chunk_key, prompt) for chunk_key, prompt in all_prompts]
        results = await asyncio.gather(*tasks)
        
        # å¤„ç†ç»“æœ
        batch_results = []
        for chunk_key, result, error in results:
            if error:
                logger.warning(f"è·³è¿‡å¤±è´¥çš„æ–‡æœ¬å— {chunk_key}: {error}")
                batch_results.append(([], []))
                continue
            
            try:
                # è§£æJSONç»“æœ
                import json
                parsed = json.loads(result)
                
                entities = []
                relationships = []
                
                # å¤„ç†å®ä½“
                if "entities" in parsed:
                    for entity in parsed["entities"]:
                        entities.append({
                            "entity_name": entity.get("name", ""),
                            "entity_type": entity.get("type", ""),
                            "description": entity.get("description", ""),
                            "chunk_key": chunk_key
                        })
                
                # å¤„ç†å…³ç³»
                if "relationships" in parsed:
                    for rel in parsed["relationships"]:
                        relationships.append({
                            "src_id": rel.get("source", ""),
                            "tgt_id": rel.get("target", ""),
                            "relation_type": rel.get("relation", ""),
                            "chunk_key": chunk_key
                        })
                
                batch_results.append((entities, relationships))
                
            except Exception as e:
                logger.warning(f"è§£æç»“æœå¤±è´¥ {chunk_key}: {e}")
                batch_results.append(([], []))
        
        return batch_results

def create_parallel_extractor(llm_func, max_concurrent=4, batch_size=8):
    """åˆ›å»ºå¹¶è¡Œå®ä½“æå–å™¨å®ä¾‹"""
    return ParallelEntityExtractor(llm_func, max_concurrent, batch_size)

# ä½¿ç”¨ç¤ºä¾‹
async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # æ¨¡æ‹ŸLLMå‡½æ•°
    async def mock_llm_func(prompt):
        await asyncio.sleep(1)  # æ¨¡æ‹ŸAPIè°ƒç”¨å»¶è¿Ÿ
        return '{"entities": [{"name": "æµ‹è¯•å®ä½“", "type": "æ¦‚å¿µ", "description": "æµ‹è¯•æè¿°"}], "relationships": []}'
    
    # åˆ›å»ºå¹¶è¡Œæå–å™¨
    extractor = create_parallel_extractor(
        llm_func=mock_llm_func,
        max_concurrent=4,
        batch_size=8
    )
    
    # æ¨¡æ‹Ÿæ–‡æœ¬å—æ•°æ®
    mock_chunks = {
        f"chunk_{i}": {"content": f"è¿™æ˜¯ç¬¬{i}ä¸ªæ–‡æœ¬å—çš„å†…å®¹"} 
        for i in range(20)
    }
    
    # æ‰§è¡Œå¹¶è¡Œæå–
    result = await extractor.extract_entities_parallel(
        chunks=mock_chunks,
        knowledge_graph_inst=None,
        entity_vdb=None,
        entity_name_vdb=None,
        relationships_vdb=None,
        global_config={}
    )
    
    print(f"æå–ç»“æœ: {result}")

if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(example_usage())
